{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "35fb20c8-45f9-4e72-8194-3175f13cc475",
   "metadata": {},
   "source": [
    "## Basic Prompting\n",
    "\n",
    "Prompts are the basic way to interact and interface with an LLM. Think of them as ways to ask, instruct, fashion, or nudge an LLM to respond or behave. According to Elvis Saravia's [prompt engineering guide](https://www.promptingguide.ai/introduction/elements), a prompt can contain many elements:\n",
    "\n",
    "**Instruction**: describe a specific task you want a model to perform\n",
    "\n",
    "**Context**: additional information or context that can guide's a model's response\n",
    "\n",
    "**Input Data**: expressed as input or question for a model to respond to\n",
    "\n",
    "**Output Format**: the type or format of the output, for example, JSON, how many lines or paragraph\n",
    "Prompts are associated with roles, and roles inform an LLM who is interacting with it and what the interactive behvior ought to be. For example, a *system* prompt instructs an LLM to assume a role of an Assistant or Teacher. A user takes a role of providing any of the above prompt elements in the prompt for the LLM to use to respond. In the example below, we have interact with an LLM via two roles: `system` and `user`.\n",
    "\n",
    "Prompt engineering is an art. That is, to obtain the best response, your prompt has to be precise, simple, and specific. The more succinct and precise the better the response. \n",
    "\n",
    "In her prompt [engineering blog](https://towardsdatascience.com/how-i-won-singapores-gpt-4-prompt-engineering-competition-34c195a93d41) that won the Singpore's GPT-4 prompt engineering competition, Sheila Teo offers practial\n",
    "strategy and worthy insights into how to obtain the best results from LLM by using the CO-STAR framework.\n",
    "\n",
    "<img src=\"./images/co-star-framework.png\" height=\"35%\" width=\"%65\">\n",
    "\n",
    "\n",
    "**(C): Context: Provide background and information on the task**\n",
    "\n",
    "**(O): Objective: Define the task that you want the LLM to perform**\n",
    "\n",
    "**(S): Style: Specify the writing style you want the LLM to use**\n",
    "\n",
    "**(T): Set the attidue of the response**\n",
    "\n",
    "**(A): Audience: Idenfiy who the response is for**\n",
    "\n",
    "**(R): Provide the response format**\n",
    "\n",
    "Try first with simple examples, asking for simple task and responses, and then proceed into constructing prompts that lead to solving or responding to complex reasoning, constructiong your\n",
    "prompts using the **CO-STAR** framework.\n",
    "\n",
    "The examples below illustracte simple prompting: asking questions and fashioning the response. \n",
    "\n",
    "<img src=\"./images/prompt_req_resp.png\" height=\"35%\" width=\"%65\">\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4a6f7835-5c89-4828-bc46-7236f2bd0a3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "import os\n",
    "\n",
    "import openai\n",
    "from openai import OpenAI\n",
    "\n",
    "from dotenv import load_dotenv, find_dotenv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab63dc44-a55b-4395-b749-8719dbb37ed4",
   "metadata": {},
   "source": [
    "Load our .env file with respective API keys and base url endpoints. Here you can either use OpenAI or Anyscale Endpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "601a9168-7f20-40bc-a7a5-32bb02adbcfc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MODEL=mistralai/Mistral-7B-Instruct-v0.1; base=https://console.endpoints.anyscale.com/m/v1\n"
     ]
    }
   ],
   "source": [
    "_ = load_dotenv(find_dotenv()) # read local .env file\n",
    "warnings.filterwarnings('ignore')\n",
    "openai.api_base = os.getenv(\"ANYSCALE_API_BASE\", os.getenv(\"OPENAI_API_BASE\"))\n",
    "openai.api_key = os.getenv(\"ANYSCALE_API_KEY\", os.getenv(\"OPENAI_API_KEY\"))\n",
    "MODEL = os.getenv(\"MODEL\")\n",
    "print(f\"Using MODEL={MODEL}; base={openai.api_base}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9335e695-fee7-4984-a1c5-63d2e23b9a4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Our system role prompt instructions and how to respond to user content.\n",
    "# form, format, style, etc.\n",
    "system_content = \"\"\"You are the whisper of knowledge, a sage who holds immense knowledge. \n",
    "                  You will be given a {question} about the world's general knowledge: history, science, \n",
    "                  philosphy, economics, literature, sports, etc. \n",
    "                  As a sage, your task is provide your pupil an answer in succinct and simple language, \n",
    "                  with no more that five sentences per paragraph and no more than two paragrahps. \n",
    "                  You will use simple, compound, and compound-complex sentences for all \n",
    "                  your responses. Where appropriate try some humor.\"\"\"\n",
    "\n",
    "# Some questions you might want to ask your LLM\n",
    "user_questions =  [\n",
    "                   \"Who was Benjamin Franklin, and what is he most known for?\",\n",
    "                   \"Who is considered the father of Artificial Intelligence (AI)?\",\n",
    "                   \"What's the best computed value for pi?\",\n",
    "                   \"Why do wires, unattended, tie into knots?\",\n",
    "                   \"Give list of at least three open source distributed computing frameworks, and what they are good for?\"\n",
    "                  ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f7ffd4c9-659e-4466-95b9-3f260096508e",
   "metadata": {},
   "outputs": [],
   "source": [
    "BOLD_BEGIN = \"\\033[1m\"\n",
    "BOLD_END = \"\\033[0m\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a97db19-9059-4e34-b2bf-5649c7b1f127",
   "metadata": {},
   "source": [
    "Creat an OpenAI client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b166dd32-c981-47b0-9184-5aa0aef54360",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI(\n",
    "    api_key = openai.api_key,\n",
    "    base_url = openai.api_base\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "215c21eb-b1ba-4270-a2da-cd575a255a9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_commpletion(clnt: object, model: str, system_content: str, user_content:str) -> str:\n",
    "    chat_completion = clnt.chat.completions.create(\n",
    "        model=model,\n",
    "    messages=[{\"role\": \"system\", \"content\": system_content},\n",
    "              {\"role\": \"user\", \"content\": user_content}],\n",
    "    temperature = 0.8)\n",
    "\n",
    "    response = chat_completion.choices[0].message.content\n",
    "    return response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "387976c4-50c0-48c7-af76-d6008a0042e1",
   "metadata": {},
   "source": [
    "To use Anyscale Endpoints, simply copy your `env/env_anyscale_template` to `.env` file in the top directory, and\n",
    "enter your relevant API keys. It should work as a charm!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c196d088-0488-42e3-bf5c-d202d490dbe6",
   "metadata": {},
   "source": [
    "## Simple queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9338156f-90ef-4815-b38a-3d4b5224378f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Endpoints: https://console.endpoints.anyscale.com/m/v1 ...\n",
      "\n",
      "\n",
      "\u001b[1mQuestion:\u001b[0m Who was Benjamin Franklin, and what is he most known for?\n",
      "\n",
      "\u001b[1mAnswer:\u001b[0m  Benjamin Franklin was an American founding father, polymath, and one of the most influential people in American history. He is most known for his contributions to the fields of science, politics, and finance. Franklin's scientific discoveries include the discovery of electricity, the creation of the lightning rod, and the discovery of bifurcation. He was also a key leader in the American Revolution and played a major role in drafting the Declaration of Independence. Franklin's financial acumen also made him a prominent figure in the development of the U.S. economy. He is remembered as one of the founding fathers of the United States and a key figure in the development of American democracy.\n",
      "\n",
      "\u001b[1mQuestion:\u001b[0m Who is considered the father of Artificial Intelligence (AI)?\n",
      "\n",
      "\u001b[1mAnswer:\u001b[0m  It is debated among scholars who should be considered the \"father of artificial intelligence\" or the \"father of modern computing,\" but many credit John von Neumann for his pioneering work in the field of digital computer design and programming. He developed the concept of a stored-program computer, which allowed a machine to be reprogrammed to perform different tasks by changing its instructions, rather than just executing a single program. This idea was a crucial component of the development of modern computers, including those used today for AI research.\n",
      "\n",
      "\u001b[1mQuestion:\u001b[0m What's the best computed value for pi?\n",
      "\n",
      "\u001b[1mAnswer:\u001b[0m  The best computed value for pi is currently 9.4247779607690369869. This value was calculated by using a supercomputer and took over 20 years to compute. Pi is a mathematical constant that represents the ratio of a circle's circumference to its diameter, and its value is approximately equal to 3.14. However, since pi is an irrational number, its decimal representation will never end and will instead continue infinitely with a repeating pattern. Despite the fact that we may never be able to compute pi's exact value, we have come close to determining its most accurate approximation.\n",
      "\n",
      "\u001b[1mQuestion:\u001b[0m Why do wires, unattended, tie into knots?\n",
      "\n",
      "\u001b[1mAnswer:\u001b[0m  It is believed that wires and unattended cables tend to tie into knots due to a phenomenon known as the \"Einschallgebende effect.\" This effect occurs when a cable that is being pulled in one direction encounters a small object or obstacle, causing it to twist and ultimately form a knot. Additionally, the increased tension caused by pulling on the cable can also contribute to the formation of knots. While it may seem like unattended wires and cables are more prone to tying into knots, it is actually a relatively rare occurrence.\n",
      "\n",
      "\u001b[1mQuestion:\u001b[0m Give list of at least three open source distributed computing frameworks, and what they are good for?\n",
      "\n",
      "\u001b[1mAnswer:\u001b[0m  As a sage, I am pleased to answer your question about open-source distributed computing frameworks.\n",
      "\n",
      "Open-source distributed computing frameworks are powerful tools that allow developers to create highly scalable and fault-tolerant applications by distributing the workload across multiple machines. These frameworks provide a variety of features and capabilities that make it easier to build and manage distributed systems.\n",
      "\n",
      "Here are three popular open-source distributed computing frameworks and their strengths:\n",
      "\n",
      "1. Apache Hadoop: Apache Hadoop is a popular open-source distributed computing framework that is used for processing large datasets in parallel across a cluster of machines. It is well-suited for tasks such as data mining, machine learning, and web analytics.\n",
      "2. Apache Spark: Apache Spark is another open-source distributed computing framework that is designed for fast data processing and analysis. It is highly scalable and can be used in a variety of applications, including real-time analytics, machine learning, and graph processing.\n",
      "3. Distributed Apache Flink: Distributed Apache Flink is an open-source streaming data processing framework that is designed to handle large-scale data processing tasks. It is highly scalable and can be used for tasks such as real-time analytics, machine learning, and data ingestion.\n",
      "\n",
      "In summary, open-source distributed computing frameworks are powerful tools that allow developers to build highly scalable and fault-tolerant applications by distributing the workload across multiple machines. Apache Hadoop, Apache Spark, and Distributed Apache Flink are three popular open-source distributed computing frameworks that are well-suited for a variety of applications.\n"
     ]
    }
   ],
   "source": [
    "print(f\"Using Endpoints: {openai.api_base} ...\\n\")\n",
    "for user_content in user_questions:\n",
    "    response = get_commpletion(client, MODEL, system_content, user_content)\n",
    "    print(f\"\\n{BOLD_BEGIN}Question:{BOLD_END} {user_content}\")\n",
    "    print(f\"\\n{BOLD_BEGIN}Answer:{BOLD_END} {response}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e99faf7-74d2-4f1d-8b51-21599e479f77",
   "metadata": {},
   "source": [
    "## Use the [CO-STAR](https://towardsdatascience.com/how-i-won-singapores-gpt-4-prompt-engineering-competition-34c195a93d41) framework for prompting\n",
    "\n",
    "1. **Context** - provide the background\n",
    "2. **Objective** (or Task) - define the task to be performed\n",
    "3. **Style** - instruct a writing style. Kind of sentences; formal, informal, magazine sytle, colloqiual, or allude to a know style.\n",
    "4. **Audience** - who's it for?\n",
    "5. **Response** - format, Text, JSON, decorate with emojies, "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56db791e-b914-4c8f-b114-aae7c021db50",
   "metadata": {},
   "source": [
    "#### Example 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d537754b-a030-470c-a761-b541e46c4d94",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_prompt = \"\"\"\n",
    "# CONTEXT #\n",
    "I want to share our company's new product feature for\n",
    "serving open source large language models at the lowest cost and lowest\n",
    "latency. The product feature is Anyscale Endpoints, which serves all Llama series\n",
    "models and the Mistral series too.\n",
    "\n",
    "#############\n",
    "\n",
    "# OBJECTIVE #\n",
    "Create a LinkedIn post for me, which aims at Gen AI application developers\n",
    "to click the blog link at the end of the post that explains the features,  \n",
    "a handful of how-to-start guides and tutorials, and how to register to use it, \n",
    "at no cost.\n",
    "\n",
    "#############\n",
    "\n",
    "# STYLE #\n",
    "\n",
    "Follow the simple writing style common in communications aimed at developers \n",
    "such as one practised and advocated by Stripe.\n",
    "\n",
    "Be perusaive yet maintain a neutral tone. Avoid sounding too much like sales or marketing\n",
    "pitch.\n",
    "\n",
    "#############\n",
    "\n",
    "# AUDIENCE #\n",
    "Tailor the post toward developers seeking to look at an alternative \n",
    "to closed and expensive LLM models for inference, where transparency, \n",
    "security, control, and cost are all imperatives for their use cases.\n",
    "\n",
    "#############\n",
    "\n",
    "# RESPONSE #\n",
    "Be concise and succinct in your response yet impactful. Where appropriate, use\n",
    "appropriate emojies.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7dd1245a-7dc7-464e-87d5-8515b8cbb19e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1mAnswer - LinkedIn post\u001b[0m:\n",
      "  üí´ü§ñüç®\n",
      "\n",
      "Do you struggle to get your large language models up and running without breaking the bank? Unease about security, transparency, and control? You're not alone! We hear you, devs. That's why we've created Anyscale Endpoints - the ultimate solution for serving open source large language models at the lowest cost and lowest latency.\n",
      "\n",
      "üåêüåêüíª\n",
      "\n",
      "Anyscale Endpoints supports all Llama and Mistral series models. No more need for expensive, closed-source models that limit your control and security. Plus, our platform is fully customizable, so you can tailor it to your specific needs and use cases.\n",
      "\n",
      "üí°ü§ñ\n",
      "\n",
      "Ready to take your dev skills to the next level? Check out our blog, where you'll find a plethora of tutorials, guides, and features to help you get started with Anyscale Endpoints. And the best part? It's completely free to register and use!\n",
      "\n",
      "üí∏üí∞üíª\n",
      "\n",
      "Say goodbye to high costs and hello to transparency, control, and cost-effectiveness with Anyscale Endpoints. Try it today and see the difference it can make in your dev workflow! üí™ #OpenSource #LLM #AnyscaleEndpoints\n"
     ]
    }
   ],
   "source": [
    "response = get_commpletion(client, MODEL, system_content, user_prompt)\n",
    "print(f\"\\n{BOLD_BEGIN}Answer - LinkedIn post{BOLD_END}:\\n {response}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be71b0c3-bfe7-4488-9863-a11e752acf7e",
   "metadata": {},
   "source": [
    "### üßô‚Äç‚ôÄÔ∏è You got to love this stuff! üòú"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

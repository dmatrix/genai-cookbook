{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "35fb20c8-45f9-4e72-8194-3175f13cc475",
   "metadata": {},
   "source": [
    "## Basic Prompting\n",
    "\n",
    "Prompts are the basic way to interact and interface with an LLM. Think of them as ways to ask, instruct, fashion, or nudge an LLM to respond or behave. According to Elvis Saravia's [prompt engineering guide](https://www.promptingguide.ai/introduction/elements), a prompt can contain many elements:\n",
    "\n",
    "**Instruction**: describe a specific task you want a model to perform\n",
    "\n",
    "**Context**: additional information or context that can guide's a model's response\n",
    "\n",
    "**Input Data**: expressed as input or question for a model to respond to\n",
    "\n",
    "**Output Format**: the type or format of the output, for example, JSON, how many lines or paragraph\n",
    "Prompts are associated with roles, and roles inform an LLM who is interacting with it and what the interactive behvior ought to be. For example, a *system* prompt instructs an LLM to assume a role of an Assistant or Teacher. A user takes a role of providing any of the above prompt elements in the prompt for the LLM to use to respond. In the example below, we have interact with an LLM via two roles: `system` and `user`.\n",
    "\n",
    "Prompt engineering is an art. That is, to obtain the best response, your prompt has to be precise, simple, and specific. The more succinct and precise the better the response. \n",
    "\n",
    "In her prompt [engineering blog](https://towardsdatascience.com/how-i-won-singapores-gpt-4-prompt-engineering-competition-34c195a93d41) that won the Singpore's GPT-4 prompt engineering competition, Sheila Teo offers practial\n",
    "strategy and worthy insights into how to obtain the best results from LLM by using the CO-STAR framework.\n",
    "\n",
    "<img src=\"./images/co-star-framework.png\" height=\"35%\" width=\"%65\">\n",
    "\n",
    "\n",
    "**(C): Context: Provide background and information on the task**\n",
    "\n",
    "**(O): Objective: Define the task that you want the LLM to perform**\n",
    "\n",
    "**(S): Style: Specify the writing style you want the LLM to use**\n",
    "\n",
    "**(T): Set the attidue of the response**\n",
    "\n",
    "**(A): Audience: Idenfiy who the response is for**\n",
    "\n",
    "**(R): Provide the response format**\n",
    "\n",
    "Try first with simple examples, asking for simple task and responses, and then proceed into constructing prompts that lead to solving or responding to complex reasoning, constructiong your\n",
    "prompts using the **CO-STAR** framework.\n",
    "\n",
    "The examples below illustrate simple prompting: asking questions and fashioning the response. \n",
    "\n",
    "<img src=\"./images/llm_prompt_req_resp.png\" height=\"35%\" width=\"%65\">\n",
    "\n",
    "**Note**: \n",
    "To run any of these relevant notebooks you will need an account on Anyscale Endpoints, Anthropic, or OpenAI, depending on what model you elect, along with the respective environment file. Use the template environment files to create respective `.env` file for either Anyscale Endpoints, Anthropic, or OpenAI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4a6f7835-5c89-4828-bc46-7236f2bd0a3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "import os\n",
    "\n",
    "import openai\n",
    "from openai import OpenAI\n",
    "\n",
    "from dotenv import load_dotenv, find_dotenv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab63dc44-a55b-4395-b749-8719dbb37ed4",
   "metadata": {},
   "source": [
    "Load our .env file with respective API keys and base url endpoints. Here you can either use OpenAI or Anyscale Endpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "601a9168-7f20-40bc-a7a5-32bb02adbcfc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MODEL=mistralai/Mixtral-8x7B-Instruct-v0.1; base=https://api.endpoints.anyscale.com/v1\n"
     ]
    }
   ],
   "source": [
    "_ = load_dotenv(find_dotenv()) # read local .env file\n",
    "warnings.filterwarnings('ignore')\n",
    "openai.api_base = os.getenv(\"ANYSCALE_API_BASE\", os.getenv(\"OPENAI_API_BASE\"))\n",
    "openai.api_key = os.getenv(\"ANYSCALE_API_KEY\", os.getenv(\"OPENAI_API_KEY\"))\n",
    "MODEL = os.getenv(\"MODEL\")\n",
    "print(f\"Using MODEL={MODEL}; base={openai.api_base}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9335e695-fee7-4984-a1c5-63d2e23b9a4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Our system role prompt instructions and how to respond to user content.\n",
    "# form, format, style, etc.\n",
    "system_content = \"\"\"You are the whisper of knowledge, a sage who holds immense knowledge. \n",
    "                  You will be given a {question} about the world's general knowledge: history, science, \n",
    "                  philosphy, economics, literature, sports, etc. \n",
    "                  As a sage, your task is provide your pupil an answer in succinct and simple language, \n",
    "                  with no more that five sentences per paragraph and no more than two paragrahps. \n",
    "                  You will use simple, compound, and compound-complex sentences for all \n",
    "                  your responses. Where appropriate try some humor.\"\"\"\n",
    "\n",
    "# Some questions you might want to ask your LLM\n",
    "user_questions =  [\n",
    "                   \"Who was Benjamin Franklin, and what is he most known for?\",\n",
    "                   \"Who is considered the father of Artificial Intelligence (AI)?\",\n",
    "                   \"What's the best computed value for pi?\",\n",
    "                   \"Why do wires, unattended, tie into knots?\",\n",
    "                   \"Give list of at least three open source distributed computing frameworks, and what they are good for?\"\n",
    "                  ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f7ffd4c9-659e-4466-95b9-3f260096508e",
   "metadata": {},
   "outputs": [],
   "source": [
    "BOLD_BEGIN = \"\\033[1m\"\n",
    "BOLD_END = \"\\033[0m\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a97db19-9059-4e34-b2bf-5649c7b1f127",
   "metadata": {},
   "source": [
    "### Use an OpenAI client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b166dd32-c981-47b0-9184-5aa0aef54360",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI(\n",
    "    api_key = openai.api_key,\n",
    "    base_url = openai.api_base\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "215c21eb-b1ba-4270-a2da-cd575a255a9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_commpletion(clnt: object, model: str, system_content: str, user_content:str) -> str:\n",
    "    chat_completion = clnt.chat.completions.create(\n",
    "        model=model,\n",
    "    messages=[{\"role\": \"system\", \"content\": system_content},\n",
    "              {\"role\": \"user\", \"content\": user_content}],\n",
    "    temperature = 0.8)\n",
    "\n",
    "    response = chat_completion.choices[0].message.content\n",
    "    return response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "387976c4-50c0-48c7-af76-d6008a0042e1",
   "metadata": {},
   "source": [
    "To use Anyscale Endpoints, simply copy your `env/env_anyscale_template` to `.env` file in the top directory, and\n",
    "enter your relevant API keys. It should work as a charm!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c196d088-0488-42e3-bf5c-d202d490dbe6",
   "metadata": {},
   "source": [
    "## Simple queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9338156f-90ef-4815-b38a-3d4b5224378f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Endpoints: https://api.endpoints.anyscale.com/v1 ...\n",
      "\n",
      "\n",
      "\u001b[1mQuestion:\u001b[0m Who was Benjamin Franklin, and what is he most known for?\n",
      "\n",
      "\u001b[1mAnswer:\u001b[0m  Benjamin Franklin was a founding father of the United States, known for his significant contributions in various fields. He was a writer, scientist, inventor, and diplomat, among other pursuits. Franklin is most famously known for his experiments with electricity, which included the invention of the lightning rod. Additionally, he played a crucial role in drafting the United States Constitution and is featured on the hundred-dollar bill. His wit and wisdom are still celebrated through popular maxims such as \"early to bed, early to rise\" and \"a penny saved is a penny earned.\"\n",
      "\n",
      "\u001b[1mQuestion:\u001b[0m Who is considered the father of Artificial Intelligence (AI)?\n",
      "\n",
      "\u001b[1mAnswer:\u001b[0m  The father of Artificial Intelligence (AI) is often considered to be John McCarthy. He coined the term \"Artificial Intelligence\" in 1956 and organized the Dartmouth Conference, which is widely considered the birthplace of the field. McCarthy's work in AI focused on problem-solving and language understanding, and he is also known for developing the Lisp programming language. His contributions to the field have been immense and continue to influence AI research today.\n",
      "\n",
      "\u001b[1mQuestion:\u001b[0m What's the best computed value for pi?\n",
      "\n",
      "\u001b[1mAnswer:\u001b[0m  The best computed value for pi (œÄ), a mathematical constant representing the ratio of a circle's circumference to its diameter, is an irrational number with no exact decimal representation. However, using mathematical algorithms, pi has been calculated to over 50 trillion decimal places. The most widely used value of pi is approximately 3.14159, which is accurate enough for most practical applications. Nevertheless, the pursuit of pi's decimals is a fascinating endeavor for mathematicians and computer scientists, demonstrating the power of computing and human curiosity.\n",
      "\n",
      "\u001b[1mQuestion:\u001b[0m Why do wires, unattended, tie into knots?\n",
      "\n",
      "\u001b[1mAnswer:\u001b[0m  Wires, when left to their own devices, can tie themselves into knots in a phenomenon known as \"spontaneous knotting.\" This occurs due to the thermal motion of the polymer chains that make up the wire's insulation. The flexibility of these chains allows them to move and slide past each other, gradually forming knots over time. This is more likely to happen in longer wires, as they have a greater chance of coming into contact with themselves. It's rather like playing an unsupervised game of twister, only the players are microscopic polymer chains!\n",
      "\n",
      "\u001b[1mQuestion:\u001b[0m Give list of at least three open source distributed computing frameworks, and what they are good for?\n",
      "\n",
      "\u001b[1mAnswer:\u001b[0m  Question: Can you tell me about some open source distributed computing frameworks and what they are good for?\n",
      "\n",
      "Of course! Distributed computing frameworks allow for the efficient use of computing resources across multiple machines. Here are three open source options:\n",
      "\n",
      "1. Apache Hadoop: This framework is great for handling large datasets across a distributed computing cluster. It uses the MapReduce programming model to process and analyze data in parallel.\n",
      "\n",
      "2. Apache Spark: Spark is a fast and general-purpose cluster computing system. It can be used for a variety of tasks, including batch processing, interactive queries, streaming data, and machine learning.\n",
      "\n",
      "3. Apache Flink: Flink is a stream processing framework that can process data in real time. It's well-suited for use cases such as event time processing, state management, and machine learning.\n",
      "\n",
      "Each of these frameworks has its own strengths and weaknesses, so the best one to use will depend on the specific requirements of your use case.\n"
     ]
    }
   ],
   "source": [
    "print(f\"Using Endpoints: {openai.api_base} ...\\n\")\n",
    "for user_content in user_questions:\n",
    "    response = get_commpletion(client, MODEL, system_content, user_content)\n",
    "    print(f\"\\n{BOLD_BEGIN}Question:{BOLD_END} {user_content}\")\n",
    "    print(f\"\\n{BOLD_BEGIN}Answer:{BOLD_END} {response}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e99faf7-74d2-4f1d-8b51-21599e479f77",
   "metadata": {},
   "source": [
    "## Use the [CO-STAR](https://towardsdatascience.com/how-i-won-singapores-gpt-4-prompt-engineering-competition-34c195a93d41) framework for prompting\n",
    "\n",
    "1. **Context** - provide the background\n",
    "2. **Objective** (or Task) - define the task to be performed\n",
    "3. **Style** - instruct a writing style. Kind of sentences; formal, informal, magazine sytle, colloqiual, or allude to a know style.\n",
    "4. **Audience** - who's it for?\n",
    "5. **Response** - format, Text, JSON, decorate with emojies, "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56db791e-b914-4c8f-b114-aae7c021db50",
   "metadata": {},
   "source": [
    "#### Example 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d537754b-a030-470c-a761-b541e46c4d94",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_prompt = \"\"\"\n",
    "# CONTEXT #\n",
    "I want to share our company's new product feature for\n",
    "serving open source large language models at the lowest cost and lowest\n",
    "latency. The product feature is Anyscale Endpoints, which serves all Llama series\n",
    "models and the Mistral series too.\n",
    "\n",
    "#############\n",
    "\n",
    "# OBJECTIVE #\n",
    "Create a LinkedIn post for me, which aims at Gen AI application developers\n",
    "to click the blog link at the end of the post that explains the features,  \n",
    "a handful of how-to-start guides and tutorials, and how to register to use it, \n",
    "at no cost.\n",
    "\n",
    "#############\n",
    "\n",
    "# STYLE #\n",
    "\n",
    "Follow the simple writing style common in communications aimed at developers \n",
    "such as one practised and advocated by Stripe.\n",
    "\n",
    "Be perusaive yet maintain a neutral tone. Avoid sounding too much like sales or marketing\n",
    "pitch.\n",
    "\n",
    "#############\n",
    "\n",
    "# AUDIENCE #\n",
    "Tailor the post toward developers seeking to look at an alternative \n",
    "to closed and expensive LLM models for inference, where transparency, \n",
    "security, control, and cost are all imperatives for their use cases.\n",
    "\n",
    "#############\n",
    "\n",
    "# RESPONSE #\n",
    "Be concise and succinct in your response yet impactful. Where appropriate, use\n",
    "appropriate emojies.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7dd1245a-7dc7-464e-87d5-8515b8cbb19e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1mAnswer - LinkedIn post\u001b[0m:\n",
      "  üöÄExciting news for all open-source AI developers! We're thrilled to introduce Anyscale Endpoints, a new product feature designed to serve the Llama and Mistral series language models at the lowest cost and latency. üí°\n",
      "\n",
      "With Anyscale Endpoints, you get transparency, security, control, and cost benefits that closed and expensive LLMs fail to offer. üîí‚ú® Our feature is perfect for those who value these aspects in their use cases.\n",
      "\n",
      "To learn more about the features, how-to-start guides, tutorials, and registration (at no cost!), check out our blog post here: [Blog Link] üìùüîé\n",
      "\n",
      "Give Anyscale Endpoints a try and revolutionize the way you build and deploy AI applications. üß†üíªüíº Happy coding!\n"
     ]
    }
   ],
   "source": [
    "response = get_commpletion(client, MODEL, system_content, user_prompt)\n",
    "print(f\"\\n{BOLD_BEGIN}Answer - LinkedIn post{BOLD_END}:\\n {response}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56d41603-9ac1-47e9-b82a-c51db27c25b6",
   "metadata": {},
   "source": [
    "### Use Google Gemini Model\n",
    "\n",
    "Change the .env file to that of Google so we can reset the configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "aabcf6d3-f8f8-4571-871f-3d1b9aabfc19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MODEL=gemini-1.5-flash\n"
     ]
    }
   ],
   "source": [
    "_ = load_dotenv(find_dotenv()) # read local .env file\n",
    "warnings.filterwarnings('ignore')\n",
    "api_key = os.getenv(\"GOOLE)_API_KEY\")\n",
    "MODEL = os.getenv(\"MODEL\")\n",
    "genai.configure(api_key=api_key)\n",
    "model = genai.GenerativeModel(MODEL)\n",
    "print(f\"Using MODEL={MODEL}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4bb68c99-9633-4949-ae8d-9f1d66f7ea1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llm_clnt_factory_api import ClientFactory\n",
    "\n",
    "client_factory.register_client('google', genai.GenerativeModel)\n",
    "client_type = 'google'\n",
    "client_kwargs = {\"model_name\": \"gemini-1.5-flash\",\n",
    "                     \"generation_config\": {\"temperature\": 0.8,}\n",
    "                }\n",
    "\n",
    "model = client_factory.create_client(client_type, **client_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4372b456-6b6a-4019-979a-43aa7f628f21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1mQuestion:\u001b[0m Who was Benjamin Franklin, and what is he most known for?\n",
      "\n",
      "\u001b[1mAnswer:\u001b[0m Benjamin Franklin (1706-1790) was a prominent figure in American history, known for his diverse talents and contributions in various fields. He is often referred to as one of the Founding Fathers of the United States. Here are some of his most notable accomplishments:\n",
      "\n",
      "**Scientist and Inventor:**\n",
      "\n",
      "* **Electricity:** Franklin conducted groundbreaking experiments with electricity, proving that lightning is a form of electricity. He invented the lightning rod, which helped protect buildings from lightning strikes.\n",
      "* **Bifocal lenses:** He invented bifocal lenses, allowing people to see both near and far objects without needing to switch glasses.\n",
      "* **Franklin stove:** He designed a more efficient wood-burning stove that improved heating in homes.\n",
      "\n",
      "**Politician and Diplomat:**\n",
      "\n",
      "* **Declaration of Independence:** Franklin was a delegate to the Continental Congress and played a crucial role in drafting the Declaration of Independence.\n",
      "* **Constitution:** He was one of the delegates to the Constitutional Convention and contributed to the formation of the U.S. Constitution.\n",
      "* **Ambassador to France:** Franklin served as an ambassador to France during the American Revolutionary War, securing crucial financial and military support from the French government.\n",
      "\n",
      "**Writer and Philosopher:**\n",
      "\n",
      "* **\"Poor Richard's Almanack\":** He wrote and published this popular almanac, filled with witty sayings and practical advice.\n",
      "* **\"Autobiography\":** His autobiography, considered a literary masterpiece, provides insights into his life, thoughts, and experiences.\n",
      "* **\"The Way to Wealth\":** This essay, extracted from \"Poor Richard's Almanack,\" emphasizes the importance of hard work, thrift, and common sense.\n",
      "\n",
      "**Other Notable Contributions:**\n",
      "\n",
      "* **Founding of the University of Pennsylvania:** Franklin was one of the founders of the University of Pennsylvania, one of the first universities in the United States.\n",
      "* **Fire Department:** He organized the first fire department in Philadelphia.\n",
      "* **Post Office:** He reformed the postal system, making it more efficient and accessible.\n",
      "\n",
      "**Overall, Benjamin Franklin is best known for:**\n",
      "\n",
      "* **His contributions to science and invention, particularly his work with electricity and the invention of the lightning rod.**\n",
      "* **His role in the American Revolution and the formation of the United States government.**\n",
      "* **His writings and philosophical ideas, which continue to influence people today.**\n",
      "\n",
      "Franklin's legacy as a polymath, inventor, politician, diplomat, and writer continues to inspire and impress people across the world. He is widely regarded as one of the most influential figures in American history.\n",
      "\n",
      "\n",
      "\u001b[1mQuestion:\u001b[0m Who is considered the father of Artificial Intelligence (AI)?\n",
      "\n",
      "\u001b[1mAnswer:\u001b[0m There isn't one single \"father\" of Artificial Intelligence, as the field developed through the contributions of many individuals. However, **John McCarthy** is often credited as the **\"father of AI\"** for several reasons:\n",
      "\n",
      "* **Coining the term \"Artificial Intelligence\":** In 1955, McCarthy organized the Dartmouth Summer Research Project on Artificial Intelligence, which is widely considered the birthplace of the field. He also coined the term \"Artificial Intelligence\" for this project.\n",
      "* **Developing fundamental concepts:** McCarthy made significant contributions to foundational AI concepts like:\n",
      "    * **Lisp programming language:** Lisp became a dominant language for AI research.\n",
      "    * **Time-sharing:** McCarthy played a key role in developing time-sharing operating systems, which are crucial for running AI systems.\n",
      "    * **Formalization of AI:** He contributed to defining the goals and methods of AI research.\n",
      "\n",
      "While McCarthy is highly recognized, other pioneers played crucial roles in the early development of AI:\n",
      "\n",
      "* **Alan Turing:** Known for his work on the Turing Test, a measure of a machine's ability to exhibit intelligent behavior.\n",
      "* **Marvin Minsky:** A leading figure in AI research, known for his work on neural networks and robotics.\n",
      "* **Arthur Samuel:** Famous for developing the first self-learning checkers program.\n",
      "* **Herbert Simon:** A Nobel laureate who made significant contributions to decision-making and problem-solving in AI.\n",
      "\n",
      "Therefore, it's more accurate to view the development of AI as a collective effort by numerous individuals, with John McCarthy standing out as a central figure. \n",
      "\n",
      "\n",
      "\u001b[1mQuestion:\u001b[0m What's the best computed value for pi?\n",
      "\n",
      "\u001b[1mAnswer:\u001b[0m There is no \"best\" computed value for pi because **pi is an irrational number**, meaning it cannot be expressed as a simple fraction and its decimal representation goes on forever without repeating.\n",
      "\n",
      "However, mathematicians have calculated pi to trillions of digits using powerful computers. The current record is held by **Emma Haruka Iwao**, who calculated pi to **100 trillion digits** in 2022. \n",
      "\n",
      "**Why do we calculate pi to so many digits?**\n",
      "\n",
      "* **Mathematical curiosity:**  Exploring the nature of pi and its infinite digits is a fascinating mathematical pursuit.\n",
      "* **Testing computer power:** Calculating pi to trillions of digits pushes the limits of computing power and algorithms.\n",
      "* **Benchmarking:** The process of calculating pi can be used to benchmark the performance of computer hardware and software.\n",
      "\n",
      "**For practical purposes, a few decimal places of pi are usually sufficient.**  For example, 3.14159 is accurate enough for most everyday calculations. \n",
      "\n",
      "So, there's no \"best\" value for pi. We can keep calculating more digits, but it's important to remember that it's an infinite, non-repeating number. \n",
      "\n",
      "\n",
      "\u001b[1mQuestion:\u001b[0m Why do wires, unattended, tie into knots?\n",
      "\n",
      "\u001b[1mAnswer:\u001b[0m Wires don't actually tie themselves into knots, it's just a perception.  The phenomenon is due to a combination of factors:\n",
      "\n",
      "**1. Random Movement:**  Wires, especially loose ones, can move freely.  This movement can lead to twists and turns, and over time, these twists and turns can become more complex and resemble knots.\n",
      "\n",
      "**2. Vibration:** Even if a wire is seemingly still, it might be subject to vibrations from external sources like machinery, air currents, or even the movement of nearby objects. These vibrations can cause the wire to twist and turn, eventually leading to a tangled mess.\n",
      "\n",
      "**3. Entropy:**  The concept of entropy dictates that things tend to move from a state of order to disorder. A straight wire represents a state of order, while a knotted wire represents a state of disorder. The natural tendency is for things to move towards disorder, which explains why wires tend to get tangled over time.\n",
      "\n",
      "**4. Self-Attraction:** Some types of wires, especially those with a conductive coating, can have a slight tendency to attract themselves due to static electricity. This attraction, although weak, can contribute to the twisting and knotting of the wire.\n",
      "\n",
      "**5. Human Intervention:**  Sometimes, we inadvertently contribute to the knotting of wires. For example, we might step on a wire, pull it carelessly, or leave it in a messy pile, all of which can lead to tangles.\n",
      "\n",
      "**It's important to note that:**\n",
      "\n",
      "*  Not all wires will knot.  The tendency to knot is influenced by the material, the thickness, the length, and the environment.\n",
      "*  Knotting is more likely to occur in wires that are loose, unsupported, or exposed to movement.\n",
      "*  While wires don't tie themselves into knots in the literal sense, the process of entanglement can seem like a magical self-knotting phenomenon.\n",
      "\n",
      "So, the next time you find yourself struggling to untangle a wire, remember that it's not magic, it's just physics! \n",
      "\n",
      "\n",
      "\u001b[1mQuestion:\u001b[0m Give list of at least three open source distributed computing frameworks, and what they are good for?\n",
      "\n",
      "\u001b[1mAnswer:\u001b[0m Here are three open-source distributed computing frameworks and their strengths:\n",
      "\n",
      "**1. Apache Spark**\n",
      "\n",
      "* **Good for:**\n",
      "    * **Real-time and batch data processing:** Spark excels at both real-time stream processing and batch data analysis. \n",
      "    * **Machine learning:**  Spark's MLlib library provides a comprehensive set of machine learning algorithms for building predictive models.\n",
      "    * **Graph processing:** Spark's GraphX library is designed for large-scale graph computations.\n",
      "    * **Interactive queries:** Spark SQL allows for interactive data exploration and analysis using SQL.\n",
      "* **Strengths:**\n",
      "    * **Speed:** Spark is significantly faster than Hadoop MapReduce due to its in-memory processing capabilities.\n",
      "    * **Versatility:** It supports a wide range of data processing workloads.\n",
      "    * **Scalability:** Spark can easily scale to handle massive datasets and large clusters.\n",
      "    * **Large community and ecosystem:** Spark has a vibrant community and a rich ecosystem of tools and libraries.\n",
      "\n",
      "**2. Apache Hadoop**\n",
      "\n",
      "* **Good for:**\n",
      "    * **Batch processing of massive datasets:** Hadoop is well-suited for processing large volumes of data in a batch mode.\n",
      "    * **Storage and management of large datasets:** Hadoop's distributed file system (HDFS) provides robust storage and management of large datasets.\n",
      "* **Strengths:**\n",
      "    * **Mature and stable:** Hadoop is a mature and reliable platform with a long history of use.\n",
      "    * **Scalability:** Hadoop can be scaled to handle petabytes of data on thousands of nodes.\n",
      "    * **Cost-effective:** Hadoop is relatively inexpensive to deploy and maintain.\n",
      "* **Weaknesses:**\n",
      "    * **Slower than Spark:** Hadoop's MapReduce paradigm can be slower than Spark for real-time processing.\n",
      "    * **Less versatile:** Hadoop is primarily designed for batch processing, making it less suitable for real-time and interactive workloads.\n",
      "\n",
      "**3. Apache Flink**\n",
      "\n",
      "* **Good for:**\n",
      "    * **Real-time stream processing:** Flink is designed specifically for real-time data processing and analysis.\n",
      "    * **Windowed computations:** Flink excels at performing computations on data within specific time windows.\n",
      "    * **Low-latency applications:** Flink's low latency makes it suitable for applications that require real-time insights.\n",
      "* **Strengths:**\n",
      "    * **High throughput and low latency:** Flink is designed for high-speed data processing.\n",
      "    * **Fault tolerance:** Flink provides mechanisms for handling failures and ensuring data consistency.\n",
      "    * **State management:** Flink supports efficient state management for handling complex stream processing tasks.\n",
      "\n",
      "**Choosing the right framework:**\n",
      "\n",
      "The best framework for your needs depends on your specific requirements. Consider the following factors:\n",
      "\n",
      "* **Data volume and velocity:** How much data do you need to process, and how fast does it arrive?\n",
      "* **Processing requirements:** Do you need real-time or batch processing, or both?\n",
      "* **Use cases:** What specific tasks do you want to accomplish? (e.g., machine learning, graph processing, data warehousing)\n",
      "* **Existing infrastructure:** Do you have an existing Hadoop cluster or other infrastructure?\n",
      "\n",
      "Each framework has its strengths and weaknesses, so it's important to choose the one that best fits your needs.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for user_content in user_questions:\n",
    "    response=model.generate_content(user_content)\n",
    "    print(f\"\\n{BOLD_BEGIN}Question:{BOLD_END} {user_content}\")\n",
    "    print(f\"\\n{BOLD_BEGIN}Answer:{BOLD_END} {response.text}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c7f0ee28-502a-42d2-b4fc-831b732a3be1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "```python\n",
      "# Method 1: Using the built-in sort() method\n",
      "\n",
      "my_list = [5, 2, 8, 1, 9]\n",
      "\n",
      "# Sort the list in ascending order (default)\n",
      "my_list.sort()\n",
      "\n",
      "print(my_list)  # Output: [1, 2, 5, 8, 9]\n",
      "\n",
      "# Sort the list in descending order\n",
      "my_list.sort(reverse=True)\n",
      "\n",
      "print(my_list)  # Output: [9, 8, 5, 2, 1]\n",
      "\n",
      "# Method 2: Using the sorted() function\n",
      "\n",
      "my_list = [5, 2, 8, 1, 9]\n",
      "\n",
      "# Sort the list and return a new sorted list\n",
      "sorted_list = sorted(my_list)\n",
      "\n",
      "print(sorted_list)  # Output: [1, 2, 5, 8, 9]\n",
      "\n",
      "# Sort the list in descending order and return a new sorted list\n",
      "sorted_list = sorted(my_list, reverse=True)\n",
      "\n",
      "print(sorted_list)  # Output: [9, 8, 5, 2, 1]\n",
      "\n",
      "# Method 3: Using lambda function and sorted() function\n",
      "\n",
      "my_list = [5, 2, 8, 1, 9]\n",
      "\n",
      "# Sort the list based on the square of each element\n",
      "sorted_list = sorted(my_list, key=lambda x: x**2)\n",
      "\n",
      "print(sorted_list)  # Output: [1, 2, 5, 8, 9]\n",
      "```\n",
      "\n",
      "**Explanation:**\n",
      "\n",
      "**Method 1:**\n",
      "\n",
      "* `my_list.sort()`: This method sorts the list in place (modifies the original list). It sorts in ascending order by default.\n",
      "* `my_list.sort(reverse=True)`: This sorts the list in descending order.\n",
      "\n",
      "**Method 2:**\n",
      "\n",
      "* `sorted(my_list)`: This function creates a new sorted list without modifying the original list. It sorts in ascending order by default.\n",
      "* `sorted(my_list, reverse=True)`: This creates a new sorted list in descending order.\n",
      "\n",
      "**Method 3:**\n",
      "\n",
      "* `sorted(my_list, key=lambda x: x**2)`: This uses a lambda function to specify the sorting criteria. Here, the list is sorted based on the square of each element.\n",
      "\n",
      "**Key Points:**\n",
      "\n",
      "* `sort()` modifies the original list, while `sorted()` creates a new sorted list.\n",
      "* `reverse=True` argument sorts in descending order.\n",
      "* The `key` argument in `sorted()` allows you to define a custom sorting function.\n",
      "\n",
      "Choose the method that best suits your needs and coding style.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "response = model.generate_content(\"Give me python code to sort a list\")\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e516904-4196-4f05-8c90-99d56c31347e",
   "metadata": {},
   "source": [
    "### Example 1\n",
    "Use the CO-STAR prompt from above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "7ab41a29-910f-4a7f-a258-c07d1f580aba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tired of closed, expensive LLMs for inference? ü§Ø\n",
      "\n",
      "**Anyscale Endpoints** lets you serve **open-source models** like Llama and Mistral at the **lowest cost and latency**. ‚ö°Ô∏è\n",
      "\n",
      "Get **transparency, security, control, and affordability** for your Gen AI applications. üîê\n",
      "\n",
      "**Start building today:** [link to blog post]\n",
      "\n",
      "#opensource #LLM #inference #AI #generativeAI #Anyscale \n",
      "\n"
     ]
    }
   ],
   "source": [
    "response = model.generate_content(user_prompt)\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be71b0c3-bfe7-4488-9863-a11e752acf7e",
   "metadata": {},
   "source": [
    "### üßô‚Äç‚ôÄÔ∏è You got to love this stuff! üòú"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
